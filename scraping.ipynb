{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python \n",
    "\n",
    "**An Introduction to Data Ingestion from the Internet &middot; March 31, 2017**\n",
    "\n",
    "## Outline \n",
    "\n",
    "1. Introduction \n",
    "2. Basic Workflow \n",
    "3. Basic Page Fetch \n",
    "4. HTTP Basics \n",
    "5. Parsing Data \n",
    "6. HTML Basics \n",
    "7. CSS Selectors \n",
    "8. Data Extraction \n",
    "9. Data Storage \n",
    "10. Scraping Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Two of the most popular ways of ingesting data from the internet are web scraping and web crawling. Scraping (done by scrapers) refers to the automated extraction of specific information from a web page. This information is often a page's text content, but it may also include the headers, the date the page was published, what links are present on the page, or any other specific information the page contains. \n",
    "\n",
    "Crawling (done by crawlers or spiders) involves the traversal of a website's link network, while saving or indexing all the pages in that network. \n",
    "\n",
    "Scraping is done with an explicit purpose of extracting specific information from a page, while crawling is done in order to obtain information about link networks within and between websites. \n",
    "\n",
    "It is possible to both crawl a website and scrape each of the pages, but only if we know what specific content we want from each page and have information about its structure in advance.\n",
    "\n",
    "![Scraping vs. Crawling](images/scraping_v_crawling.png)\n",
    "\n",
    "### What is Web Scraping?\n",
    "\n",
    " - Automated extraction of specific information from a web page. \n",
    " - Often a page's text content, but it may also include: \n",
    "     - Headers\n",
    "     - Date the page was published\n",
    "     - Links are present on the page\n",
    "     - Any other specific information the page contains\n",
    " - Objective: extracting specific information from a page\n",
    "\n",
    "#### Challenges of Web Scraping\n",
    "\n",
    " - Need to determine what information you want\n",
    " - Need custom scraper for each site\n",
    " - Different pages have different structure\n",
    " - Page structure/content changes periodically\n",
    " - Javascript can make scraping difficult\n",
    " - Potential legal issues\n",
    "\n",
    "\n",
    "### What is Web Crawling?\n",
    "\n",
    " - Traversal of a website's link network\n",
    " - Saving or indexing all the pages in that network\n",
    " - Obtain information about link networks within and between websites.\n",
    "\n",
    "\n",
    "#### Challenges of Web Crawling\n",
    "\n",
    " - Need to know the site structure in advance\n",
    " - Determining depth of crawl\n",
    " - Latency/bandwidth variations\n",
    " - Site mirrors and duplicate pages\n",
    " - Spider/crawler traps\n",
    "\n",
    "### From Crawling to Scraping\n",
    "\n",
    " - Different Objectives\n",
    "     - Scraping - extracting specific information from a page.\n",
    "     - Crawling - obtain information about link networks within and between websites.\n",
    "\n",
    " - Possible to crawl a site and scrape pages.\n",
    " - Need to know specific content we want from each page .\n",
    " - Need to have information about site structure in advance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflow \n",
    "\n",
    "Create a function that takes as input a url and returns data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    # Perform a web request \n",
    "    # Perform data extraction \n",
    "    # Handle or raise exceptions \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple function is then operationalized across an entire site or sites and can be parallelized for better performance. Visually:\n",
    "\n",
    "![Basic Workflow](images/workflow.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
