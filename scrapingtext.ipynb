{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python\n",
    "\n",
    "March 31, 2017\n",
    "\n",
    "## Outline\n",
    "\n",
    " - Introduction\n",
    " - Data sources\n",
    " - Types of available data \n",
    " - RESTful APIs\n",
    " - Scraping\n",
    " - Crawling\n",
    " - Useful tools\n",
    " - Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data from the Web \n",
    " \n",
    "Your mission is to get the data you need to the job you need to do.\n",
    "\n",
    "However:\n",
    "- Data is designed for operations, not analysis.\n",
    "- Data used in analysis usually needs to be denormalized.\n",
    "- There can be many gatekeepers.\n",
    "\n",
    "So what makes a good data source?\n",
    "\n",
    "As data scientists, we rely heavily on structure and patterns, not only in the content of our data, but in its history and provenance. In general, good data sources have a determinable structure, where different pieces of content are organized according to some schema and can be extracted systematically via the application of some logic to that schema. If there is no common structure or schema between documents, it becomes difficult to discern any patterns for extracting the information we want, which often results in either no data retrieved at all or significant cleaning required to correct what the ingestion process got wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publicly Available Datasets\n",
    "\n",
    " - [Amazon S3 Cloud Public Datasets](https://aws.amazon.com/datasets/)\n",
    " - [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    " - [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets)\n",
    " - [More Datasets](http://rs.io/100-interesting-data-sets-for-statistics/)\n",
    " - [Kaggle](https://www.kaggle.com/)\n",
    " - [Data.gov](https://www.data.gov/)\n",
    " - [Sunlight Foundation](https://sunlightfoundation.com/)\n",
    "\n",
    "Strategy: look for academic data sets that implement techniques that you’re interested in - these may lead you to initial data or other primary sources.\n",
    "\n",
    "Also, we’re in DC - Data.gov is a very important resource for data collection and aggregation - with APIs that are constantly being updated with new data. More importantly, Federal agencies in this area are desperate for community data work and visualizations - there are reverse pitches and more to get data scientists involved. Also keep in mind that Data.gov is just a start - the Federal Reserve Board has massive amounts of data, but cannot participate in Data.gov.\n",
    "\n",
    "**In the end, the best data is always the data you gather yourself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of Data in the Wild\n",
    "\n",
    "Data comes from a variety of sources in a format that was intended for the producer; not necessarily as you require it. Once you have it stored locally you can wrangle it to your needs and input it into a database.\n",
    "\n",
    "### Common Data Formats\n",
    "\n",
    " - CSV: stores tabular data in plain text where each row is a record and the values are delimited by commas.\n",
    " - JSON: a data-interchange format that is easy for humans to read and write and for machines to parse and generate.\n",
    " - XML: a markup language designed to carry data, with a focus on what the data is.\n",
    " - HTML: a markup language designed to display data, with a focus on how the data looks.\n",
    " \n",
    "### Serialization\n",
    "\n",
    " - Converting structured data into format to be shared, stored, or updated \n",
    " - Original structure can be restored. \n",
    " - Minimizes the size of the data so that it takes up less disk space when stored or bandwidth when shared.\n",
    " - .write()\n",
    "\n",
    "### Parsing\n",
    "\n",
    " - Processing input into meaningful structures to extract information.\n",
    " - Examples:\n",
    "     - A student parses a sentence into subject, verb, and object.\n",
    "     - A compiler parses source code.\n",
    "     - A CSV parser reads a stream according to rules (comma delimiters, quoting, etc) to extract the data in each line of a file.\n",
    " - .read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "\n",
    "Although computer scientists are used to APIs; most of the time APIs refer to Web APIs now - and this is essentially a data ingestion topic.\n",
    "\n",
    "    “In the simplest terms, APIs are sets of requirements that govern how one application can talk to another. APIs aren't at all new; whenever you use a desktop or laptop, APIs are what make it possible to move information between programs.\"\n",
    "\n",
    "    These days, APIs are especially important because they dictate how developers can create new apps that tap into big Web services—social networks like Facebook or Pinterest, for instance, or utilities like Google Maps or Dropbox. The developer of a game app, for instance, can use the Dropbox API to let users store their saved games in the Dropbox cloud instead of working out some other cloud-storage option from scratch.\n",
    "\n",
    "    Viewed more broadly, though, APIs make possible a sprawling array of Web-service \"mashups,\" in which developers use mix and match APIs from the likes of Google or Facebook or Twitter to create entirely new apps and services. In many ways, the widespread availability of APIs for major services is what's made the modern Web experience possible.”\n",
    "\n",
    "http://readwrite.com/2013/09/19/api-defined\n",
    "\n",
    "### Examples\n",
    "\n",
    " - Twitter\n",
    " - Amazon\n",
    " - Soundcloud\n",
    " - Goodreads\n",
    " - Weather Underground\n",
    " - Wordnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST\n",
    "\n",
    "REST is a simple way to organize interactions between independent systems.\n",
    "\n",
    "REST allows you to interact with minimal overhead with clients as diverse as mobile phones and other websites. In theory, REST is not tied to the web, but it's almost always implemented as such, and was inspired by HTTP. As a result, REST can be used wherever HTTP can.\n",
    "\n",
    "So what is HTTP?\n",
    "\n",
    "## HTTP Basics\n",
    "\n",
    " - HyperText Transfer Protocol\n",
    " - Foundation of data communication on the web\n",
    " - Send request, receive response\n",
    " - HTTP Request Methods\n",
    "     - GET\n",
    "     - HEAD\n",
    "     - POST\n",
    "     - PUT\n",
    "     - DELETE\n",
    " - User Agent String - browser, OS, and other system info.\n",
    " - HTTP Status Codes\n",
    "     - 1xx - Informational\n",
    "     - 2xx - Success\n",
    "     - 3xx - Redirection\n",
    "     - 4xx - Client Error\n",
    "     - 5xx - Server Error\n",
    " - TLS - successor to SSL that provides protocol for secure communications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and Crawling\n",
    "\n",
    "Two of the most popular ways of ingesting data from the internet are web scraping and web crawling. Scraping (done by scrapers) refers to the automated extraction of specific information from a web page. This information is often a page's text content, but it may also include the headers, the date the page was published, what links are present on the page, or any other specific information the page contains. \n",
    "\n",
    "Crawling (done by crawlers or spiders) involves the traversal of a website's link network, while saving or indexing all the pages in that network. \n",
    "\n",
    "Scraping is done with an explicit purpose of extracting specific information from a page, while crawling is done in order to obtain information about link networks within and between websites. \n",
    "\n",
    "It is possible to both crawl a website and scrape each of the pages, but only if we know what specific content we want from each page and have information about its structure in advance.\n",
    "\n",
    "### What is Web Scraping?\n",
    "\n",
    " - Automated extraction of specific information from a web page. \n",
    " - Often a page's text content, but it may also include: \n",
    "     - Headers\n",
    "     - Date the page was published\n",
    "     - Links are present on the page\n",
    "     - Any other specific information the page contains\n",
    " - Objective: extracting specific information from a page\n",
    "\n",
    "#### Challenges of Web Scraping\n",
    "\n",
    " - Need to determine what information you want\n",
    " - Need custom scraper for each site\n",
    " - Different pages have different structure\n",
    " - Page structure/content changes periodically\n",
    " - Javascript can make scraping difficult\n",
    " - Potential legal issues\n",
    "\n",
    "\n",
    "### What is Web Crawling?\n",
    "\n",
    " - Traversal of a website's link network\n",
    " - Saving or indexing all the pages in that network\n",
    " - Obtain information about link networks within and between websites.\n",
    "\n",
    "\n",
    "#### Challenges of Web Crawling\n",
    "\n",
    " - Need to know the site structure in advance\n",
    " - Determining depth of crawl\n",
    " - Latency/bandwidth variations\n",
    " - Site mirrors and duplicate pages\n",
    " - Spider/crawler traps\n",
    "\n",
    "### From Crawling to Scraping\n",
    "\n",
    " - Different Objectives\n",
    "     - Scraping - extracting specific information from a page.\n",
    "     - Crawling - obtain information about link networks within and between websites.\n",
    "\n",
    " - Possible to crawl a site and scrape pages.\n",
    " - Need to know specific content we want from each page .\n",
    " - Need to have information about site structure in advance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "### Requests\n",
    "\n",
    "Elegant, simple HTTP library for Python\n",
    "\n",
    "How it works:\n",
    " - Make a request to a web page (get, post, put, etc.)\n",
    " - Receive a response from server\n",
    " - Read content of server response\n",
    " - Headers\n",
    " - Cookies\n",
    " - Content\n",
    " - Etc.\n",
    "\n",
    "\n",
    "### Scrapy\n",
    "\n",
    "Open source framework for crawling websites and extracting structured data. \n",
    "\n",
    " - Spiders - define how a certain site (or group of sites) will be scraped.\n",
    " - Selectors - select certain parts of the HTML document.\n",
    " - Items - objects that serve as simple containers used to collect the scraped data. \n",
    " - Scrapy Shell - debug scraping code quickly without having to run spider. \n",
    " - Pipelines, extractors, and more!\n",
    "\n",
    "\n",
    "*For more advanced crawling and scraping, it may be worth looking into the following tools.*\n",
    "\n",
    "* Selenium - a Python library that allows you to simulate user interaction with a website.\n",
    "* Apache Nutch - a highly extensible and scalable open source web crawler.\n",
    "\n",
    "### Databases and Database Tools\n",
    "\n",
    "** WORM STORAGE **\n",
    "\n",
    " - PostgreSQL\n",
    " - Postgres App\n",
    " - Pgadmin\n",
    " - SQLite - lightweight, self-contained SQL database engine.\n",
    " - Psycopg2\n",
    " - Postico\n",
    " - Postman\n",
    " - JetBrains Database Navigator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being a Good Citizen\n",
    "\n",
    " - Robot.txt files - tell you what the site does and does not allow from crawlers.\n",
    " - Rate limiting - limiting the frequency at which you ping a website.\n",
    " - Too much traffic too quickly may bring down a smaller website.\n",
    " - Larger websites may block your IP address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
