{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data from the Web \n",
    " \n",
    "Your mission is to get the data you need to the job you need to do.\n",
    "\n",
    "However:\n",
    "- Data is designed for operations, not analysis.\n",
    "- Data used in analysis usually needs to be denormalized.\n",
    "- There can be many gatekeepers.\n",
    "\n",
    "So what makes a good data source?\n",
    "\n",
    "As data scientists, we rely heavily on structure and patterns, not only in the content of our data, but in its history and provenance. In general, good data sources have a determinable structure, where different pieces of content are organized according to some schema and can be extracted systematically via the application of some logic to that schema. If there is no common structure or schema between documents, it becomes difficult to discern any patterns for extracting the information we want, which often results in either no data retrieved at all or significant cleaning required to correct what the ingestion process got wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publicly Available Datasets\n",
    "\n",
    " - [Amazon S3 Cloud Public Datasets](https://aws.amazon.com/datasets/)\n",
    " - [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    " - [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets)\n",
    " - [More Datasets](http://rs.io/100-interesting-data-sets-for-statistics/)\n",
    " - [Kaggle](https://www.kaggle.com/)\n",
    " - [Data.gov](https://www.data.gov/)\n",
    " - [Sunlight Foundation](https://sunlightfoundation.com/)\n",
    "\n",
    "Strategy: look for academic data sets that implement techniques that you’re interested in - these may lead you to initial data or other primary sources.\n",
    "\n",
    "Also, we’re in DC - Data.gov is a very important resource for data collection and aggregation - with APIs that are constantly being updated with new data. More importantly, Federal agencies in this area are desperate for community data work and visualizations - there are reverse pitches and more to get data scientists involved. Also keep in mind that Data.gov is just a start - the Federal Reserve Board has massive amounts of data, but cannot participate in Data.gov.\n",
    "\n",
    "**In the end, the best data is always the data you gather yourself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST\n",
    "\n",
    "REST is a simple way to organize interactions between independent systems.\n",
    "\n",
    "REST allows you to interact with minimal overhead with clients as diverse as mobile phones and other websites. In theory, REST is not tied to the web, but it's almost always implemented as such, and was inspired by HTTP. As a result, REST can be used wherever HTTP can.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Scrapy\n",
    "\n",
    "Open source framework for crawling websites and extracting structured data. \n",
    "\n",
    " - Spiders - define how a certain site (or group of sites) will be scraped.\n",
    " - Selectors - select certain parts of the HTML document.\n",
    " - Items - objects that serve as simple containers used to collect the scraped data. \n",
    " - Scrapy Shell - debug scraping code quickly without having to run spider. \n",
    " - Pipelines, extractors, and more!\n",
    "\n",
    "\n",
    "*For more advanced crawling and scraping, it may be worth looking into the following tools.*\n",
    "\n",
    "* Selenium - a Python library that allows you to simulate user interaction with a website.\n",
    "* Apache Nutch - a highly extensible and scalable open source web crawler.\n",
    "\n",
    "### Databases and Database Tools\n",
    "\n",
    "** WORM STORAGE **\n",
    "\n",
    " - PostgreSQL\n",
    " - Postgres App\n",
    " - Pgadmin\n",
    " - SQLite - lightweight, self-contained SQL database engine.\n",
    " - Psycopg2\n",
    " - Postico\n",
    " - Postman\n",
    " - JetBrains Database Navigator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being a Good Citizen\n",
    "\n",
    " - Robot.txt files - tell you what the site does and does not allow from crawlers.\n",
    " - Rate limiting - limiting the frequency at which you ping a website.\n",
    " - Too much traffic too quickly may bring down a smaller website.\n",
    " - Larger websites may block your IP address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
